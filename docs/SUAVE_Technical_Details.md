# SUAVE 技术说明

## 1. 核心理念：面向临床的“三能一体”模型
SUAVE（Supervised, Unified, Augmented Variational Embedding）旨在为临床智能提供可落地的生成—判别统一框架，其核心动机体现为“三能一体”：

1. **临床预测模型构建能力** —— 通过监督头利用潜空间表征，面向死亡率、再入院风险、并发症预测等任务提供可靠概率输出。
2. **异质数据合成能力** —— 统一处理实值、分类、计数、序数等多模态表格特征，以生成隐私友好的合成数据，支持数据共享、隐私保护和数据增强。
3. **内生可解释性能力** —— 利用层次化潜变量、显式分布参数与特征级掩码，使潜在空间和重建因子可以直接映射到临床语义，满足临床应用和研究对可解释性的需求。

## 2. 设计思路

### 2.1 Schema-First 工作流
- 用户以 `Schema` 声明每个特征的类型（`real`、`cat`、`pos`、`count`、`ordinal` 等）及类别数量。
- 预处理管线使用 Schema 完成标准化/嵌入、缺失值掩码与后处理恢复，形成统一的数据接口。

### 2.2 分阶段训练调度
1. **Warm-up**：仅优化生成模型，逐步增加 KL 权重（KL annealing）稳定训练。
2. **Head 训练**：冻结生成器，训练分类头以最小化监督损失。
3. **联合微调**：小学习率联合优化生成器与分类头，实现更好的生成—判别折衷。
4. **Decoder Refine**：在监督模式下追加的解码器/先验细化阶段，可配置训练轮次（`decoder_refine_epochs`）和模式（`decoder_refine_mode`）。
   - `decoder_only`：冻结先验，仅以 warm-up 学习率对解码器做额外梯度更新。
   - `decoder_prior`：同时更新解码器与先验，并对先验参数添加 L2 锚点（`decoder_refine_prior_lambda`）以防漂移。
   - `prior_em_only`：跳过梯度步骤，基于责任度量执行一次闭式 EM/矩匹配更新，然后直接进入缓存统计。
   - `prior_em_decoder`：先执行上述闭式先验更新，再在冻结先验的前提下细化解码器。
   该阶段沿用 warm-up 的学习率与早停耐心，监控验证集 ELBO 并在结束时恢复最佳 checkpoint，从而避免对训练数据过拟合。

### 2.3 自洽的校准与评估
- 内置温度缩放（Temperature Scaling）对 logits 进行后验校准。
- 提供 AUROC、AUPRC、Brier Score、ECE 等评估函数，确保预测可靠性。

## 3. 关键创新点

| 维度 | 创新描述 |
| ---- | -------- |
| **统一 Schema 驱动的异质数据建模** | 通过显式 Schema 与类型特定的解码头（高斯、对数正态、泊松、分类 softmax 等），无缝覆盖多种特征分布，并保持缺失值一致性。 |
| **生成与监督的同域潜空间** | 层次化潜变量共享于合成与分类任务，支持条件采样、特征重建以及概率预测的一致表示。 |
| **面向临床的解释性路径** | 潜变量分量与参数直接对应到特征簇，结合互信息、KS、MMD 等指标形成可审计报告。 |
| **多阶段训练 + 校准的一站式实现** | 从预处理、训练、评估到合成数据质量审计均有统一 API，降低临床数据科学家的上手门槛。 |

## 4. 模型架构

### 4.1 组件概览
1. **Encoder (`modules/encoder.py`)**：对不同类型特征进行嵌入后，经多层感知机输出潜变量的均值 $\mu$ 与尺度参数 $\sigma$。
2. **Latent Prior**：采用多分量高斯先验或标准正态先验 $p(z)$，支持临床群体的亚群划分。
3. **Decoder (`modules/decoder.py`)**：针对每类特征输出参数（实值的均值/方差、分类概率、泊松率等），并在有缺失掩码的情况下执行重建。
4. **Supervised Head (`modules/heads.py`)**：在潜空间上添加线性或多层感知机分类器。
5. **Calibration (`modules/calibrate.py`)**：对分类头 logits 进行温度缩放。
6. **Evaluation (`evaluate.py`)**：计算性能与隐私指标。

### 4.2 前向与训练流程
- **编码**： $x$ 经标准化与缺失掩码处理后输入编码器，得到潜在分布 $q_\phi(z \mid x)$。
- **采样与重构**：从 $q_\phi(z \mid x)$ 中采样 $z$，经解码器生成参数化分布 $p_\theta(x \mid z)$ 并计算重建损失。
- **监督预测**：同一 $z$ 作为分类头输入，输出 logits 并计算监督损失与校准。
- **合成数据**：从先验 $p(z)$ 采样，经解码器生成合成样本；在条件采样时，利用分类头的逆向约束或条件先验采样特定类别的 $z$。

## 5. 数学公式

### 5.1 潜变量后验与先验
编码器给出潜变量的均值 $\mu_\phi(x)$ 与对数方差 $\log \sigma_\phi^2(x)$，近似后验为：

$$
q_\phi(z \mid x) = \mathcal{N}\big(z;\, \mu_\phi(x), \mathrm{diag}(\sigma_\phi^2(x))\big).
$$

**如何理解？** 在变分自编码器框架下，编码器可以被视为一台对患者状态进行测量的科学仪器。给定原始特征 $x$（如化验指标、生命体征等），它输出潜在向量 $z$ 的后验近似，并用均值与方差刻画这一估计的不确定性。
- $\mu_\phi(x)$ 对应后验分布的中心位置，可看作对“最典型潜在表示”的点估计。
- $\sigma^2_\phi(x)$ 体现模型在各潜在维度上的置信度：方差越大，说明该维度的估计存在越多模糊性。
- 潜在变量 $z$ 是生成模型内部的“隐藏因子”，其取值由后验分布抽样得到，用于驱动解码器重建数据。
- 参数符号 $\phi$ 表明这些均值与方差是由编码器的可学习权重共同决定的。
- $\mathcal{N}(\cdot)$ 表示高斯分布，`diag` 强调我们采用独立方差的假设，即不同潜在维度之间暂不建模协方差结构。
**什么是潜空间？** 可以把潜空间看作是对原始高维临床指标进行“压缩后”的抽象坐标系，它保留了对解码器和分类器有用的信息，同时弱化噪声与冗余。物理上，它类似于在实验物理或信号处理中常见的“特征子空间”：不同维度可以对应到潜在的病理因素、治疗反应轨迹或生理状态轴。虽然这些维度并非直接可观测量，但通过解码器的重建与分类头的预测，它们间接反映了对原始变量的影响，可用于生成合成样本或进行条件分析。
因此，模型并不会将每个样本映射为单个确定的潜向量，而是描述成围绕均值 $\mu$ 波动、尺度由 $\sigma$ 控制的概率分布，以便在后续阶段通过采样传播不确定性。

若使用混合先验，则：


$$
p(z) = \sum_{k=1}^K \pi_k \mathcal{N}(z; \mu_k, \Sigma_k), \qquad \sum_k \pi_k = 1.
$$

**如何理解？** 该式给出了模型对潜在空间的先验假设。可以将潜在空间比作对整个人群的结构化地图：
- 总和符号 $\sum_{k=1}^K$ 表示我们将所有 $K$ 个子群体的贡献叠加起来，得到完整的人群先验分布。
- 每个分量 $k$ 对应一个潜在子群体，其分布由高斯核 $\mathcal{N}(z; \mu_k, \Sigma_k)$ 描述。均值 $\mu_k$ 给出该子群体的中心，协方差 $\Sigma_k$ 则刻画其中样本的扩散范围。
- 权重 $\pi_k$ 表示新样本落入该子群体的先验概率，所有权重之和约束为 1，使该混合分布成立。
- 先验密度 $p(z)$ 综合了所有子群体的影响，为解码器提供采样时的基准分布。

与单峰正态先验相比，混合先验允许模型在潜在空间内形成多个“聚集区”，从而更好地表达临床数据可能由多个病程阶段或亚型构成的事实。
**特征关联在何处建模？** 尽管编码器后验在每个样本的条件下使用对角协方差（便于稳定优化），但整个体系仍通过以下途径捕捉特征之间的依赖关系：
- 混合先验中的协方差矩阵 $\Sigma_k$ 允许在总体人群尺度上建模潜变量维度之间的线性相关性，从而在采样时保留特征共现模式。
- 解码器针对不同类型特征输出的参数都是潜向量 $z$ 的函数，即使后验是对角的，多个特征也通过共同的潜表示隐式耦合——当潜空间的某个维度发生变化时，会同时影响多个重建分布的参数。
- 在 warm-up 之后的监督和联合阶段，分类损失提供跨特征的梯度信号，使得与标签相关的一组特征会共同调整潜空间的同一方向，进一步编码统计依赖。

#### 5.1.1 解码器细化阶段的先验 EM 更新

当选择 `prior_em_only` 或 `prior_em_decoder` 模式时，SUAVE 会依据 `_collect_posterior_statistics` 产生的责任度量 $\gamma_{nk}$、后验均值 $m_{nk}$ 与对数方差 $\log v_{nk}$ 对混合先验做一次闭式矩匹配更新。令 $N$ 为样本总数，则新的混合权重、均值和方差按下式计算：

$$
\pi_k^{\text{new}} = \frac{1}{N} \sum_{n=1}^N \gamma_{nk}, \qquad
\mu_k^{\text{new}} = \frac{\sum_{n=1}^N \gamma_{nk} m_{nk}}{\sum_{n=1}^N \gamma_{nk}}, \qquad
\sigma_{k, d}^{2,\text{new}} = \frac{\sum_{n=1}^N \gamma_{nk} \left( e^{\log v_{nk,d}} + m_{nk,d}^2 \right)}{\sum_{n=1}^N \gamma_{nk}} - (\mu_{k,d}^{\text{new}})^2.
$$

上述公式分别对混合权重做归一化、对先验均值执行加权平均，并通过“二阶矩减去均值平方”的方式获得对角方差，实现与责任统计的一次矩匹配。在实现中会对权重和方差加以截断（`\ge 1\times10^{-6}`）以避免数值不稳定，所得的 $\pi_k^{\text{new}}$、$\mu_k^{\text{new}}$、$\log \sigma_{k}^{2,\text{new}}$ 直接覆盖先验参数。

若选择 `decoder_prior` 模式，则除了常规的梯度下降之外，还会对每个先验参数引入 L2 锚定项 $\lambda \lVert \theta - \theta^{(0)} \rVert_2^2$（$\theta^{(0)}$ 为阶段起始快照，$\lambda = \text{decoder\_refine\_prior\_lambda}$），确保细化过程不会破坏既有的潜在聚类结构。



### 5.2 证据下界（ELBO）
SUAVE 的无监督训练目标采用掩码感知的 ELBO：


$$
\mathcal{L}_{\text{ELBO}}(x) = \mathbb{E}_{q_\phi(z \mid x)}\big[\log p_\theta(x_{\text{obs}} \mid z)\big] - \beta D_{\mathrm{KL}}\big(q_\phi(z \mid x) \parallel p(z)\big),
$$

其中 $x_{\text{obs}}$ 表示通过掩码选择的观测特征， $\beta$ 是 KL 退火系数。对于实值特征，重建项为高斯似然：


$$
\log p_\theta(x^{(r)} \mid z) = -\frac{1}{2}\sum_i m_i \left[ \frac{\big(x^{(r)}_i - \mu^{(r)}_{\theta,i}(z)\big)^2}{\sigma^{2,(r)}_{\theta,i}(z)} + \log \sigma^{2,(r)}_{\theta,i}(z) + \log(2\pi) \right],
$$

其中 $m_i$ 为缺失掩码。分类特征使用 softmax 分布，计数特征使用泊松或负二项分布，以此类推。

**如何理解 ELBO？** 证据下界兼顾了重建准确性与先验一致性，是无监督阶段优化的核心目标。
- 第一项 $\mathbb{E}_{q_\phi(z \mid x)}[\log p_\theta(x_{\text{obs}} \mid z)]$ 度量在给定潜变量样本的情况下，解码器生成观测特征的对数似然。其值越高，说明模型能够在保持缺失掩码一致的前提下，忠实地还原原始数据。
- 符号 $\mathbb{E}_{q_\phi(z \mid x)}$ 表示对后验分布进行期望，即考虑所有可能的潜向量并取平均。
- 第二项 $D_{\mathrm{KL}}(q\,\Vert\,p)$ 比较后验近似与先验分布的差异。若该项较大，意味着编码器输出的分布偏离了我们事先设定的潜空间结构。
- 系数 $\beta$ 控制两者的相对重要性。实践中常采用逐步增大的 KL 退火策略，使模型先聚焦于学习良好的重建，再逐渐约束潜在表示遵守先验假设。$\beta$ 越大，后验被迫更接近先验，潜空间更“整洁”（潜变量的均值接近零、方差接近 1），但若过大可能导致解码器忽视细粒度特征、产生欠拟合；$\beta$ 越小，重建项主导，潜空间可表达更多细节，但生成质量与泛化能力可能下降。该权衡与 $\beta$-VAE 框架中的设计一致：$\beta>1$ 有助于鼓励潜空间因子化、提升可解释性，而 $\beta<1$ 则让模型更偏向重建精度。SUAVE 默认的 $\beta$ 设定遵循这一实践，并可直接移植 $\beta$-VAE 文献中的调参经验。

**实值特征的重建项** 来自对数高斯似然，可按以下角度理解：
- 系数 $\frac{1}{2}$ 出现在所有正态分布的对数似然中，确保损失的量纲与概率密度匹配。
- $\big(x^{(r)}_i - \mu^{(r)}_{\theta,i}(z)\big)^2$ 是预测均值与真实值之间的平方误差，对偏差越大的样本施加更大惩罚。
- $\sigma^{2,(r)}_{\theta,i}(z)$ 提供了解码器对该特征的方差估计。若模型认为该特征本身波动较大，就会允许较大的误差而不会显著降低似然。
- 掩码 $m_i$ 使得缺失条目被排除在损失之外，避免模型凭空猜测未观测值。
- 对数项 $\log \sigma^{2,(r)}_{\theta,i}(z)$ 与常数 $\log(2\pi)$ 构成高斯分布的归一化部分，指出方差越小、常数项越负，代表模型对该特征越有把握。
- 求和符号 $\sum_i$ 表示对所有实值特征维度累计这些贡献，再整体评估重建质量。
这一拆解展示了 ELBO 如何同时关注预测准确度与不确定性量化。

#### 5.2.1 实现细节：损失聚合与归一化
- **重构项的聚合方式**：每个特征头都会返回按样本排列的对数似然向量，`sum_reconstruction_terms` 将它们在特征维度求和，因此重构分数在一个样本内部是对所有已观测特征对数似然的直接求和。【F:suave/modules/losses.py†L10-L23】掩码张量在构建这些似然时已经排除了缺失列，所以未观测条目不会对重构贡献造成偏差。
- **KL 分解与缩放**：SUAVE 单独计算类别分配的 KL (`categorical_kl`) 与连续潜变量的 KL (`gaussian_kl`)，两者相加后乘以当前的 `beta_scale`，再从重构总和中扣除；实现上使用 `total_kl = beta_scale * total_kl_unscaled` 的形式维持退火机制。【F:suave/model.py†L1568-L1600】
- **批次归一化策略**：闭式表达中的重构—KL 差值对每个样本得到的 ELBO 先求和，再对批次大小取平均（`loss = -elbo_value.mean()`），从而使优化器看到的目标是“每样本”的负 ELBO；训练循环会按样本计数将各项累加并除以样本数，确保日志指标也以平均每样本的尺度呈现。【F:suave/model.py†L1601-L1647】【F:suave/model.py†L1915-L1970】
- **优点**：这种“按样本求和、按批次取平均”的做法与变分下界的理论定义一致，既方便解释为 sum(log p_theta(x)) 减去 KL，又避免额外的经验性权重；在掩码的帮助下还可以自然处理不规则缺失。
- **潜在缺点与权衡**：由于每个样本的重构分数是所有观测特征对数似然的合计，拥有更多观测列或取值尺度较大的模态会在 ELBO 中占更大权重；若希望“每列”贡献均衡，需要额外对重构项按观测数或模态维度进行归一化，这会改变目标函数与对数似然的量纲，并可能破坏与 β 退火策略的直观对应。类似地，KL 的线性缩放虽然便于调参，但会让潜空间约束强度完全由单个超参数控制；替代方案是分别为类别与连续 KL 设定不同的缩放或利用自动调节比率（如 adaptive KL control），代价是需要更多监控指标与调参工作。

### 5.3 监督损失与温度校准
对于带标签的样本 $(x, y)$，分类头产生 logits $f_\psi(z)$，监督损失为：

$$
\mathcal{L}_{\text{sup}}(x, y) = - \sum_{c=1}^C y_c \log \frac{\exp(f_{\psi,c}(z))}{\sum_{c'=1}^C \exp(f_{\psi,c'}(z))}.
$$

温度缩放在校准阶段优化标量 $T>0$。缩放后的类别概率和目标函数写为：

$$
\hat{y}_c = \frac{\exp(f_{\psi,c}(z) / T)}{\sum_{c'=1}^C \exp(f_{\psi,c'}(z) / T)}, \qquad \mathcal{J}_{\text{cal}}(T) = - \sum_{(x,y)\in \mathcal{D}_{\text{val}}} y^\top \log \hat{y}.
$$

校准温度由 $T^* = \underset{T>0}{\mathrm{argmin}}\, \mathcal{J}_{\text{cal}}(T)$ 给出。

**如何理解监督损失？** 该损失函数即多类别交叉熵，是监督学习中常用的对数似然目标。
- $f_{\psi,c}(z)$ 表示分类头针对类别 $c$ 的 logit，是 softmax 归一化前的原始得分。
- 指数函数 $\exp(\cdot)$ 将 logit 转换为非负值，分母 $\sum_{c'} \exp(f_{\psi,c'}(z))$ 负责把所有类别归一化成概率。
- 真实标签 $y_c$ 采用独热编码，因而当某个类别为真实类别时，损失要求对应的预测概率接近 1；若偏离则产生较大惩罚。
- 负号和求和符号 $-\sum_{c=1}^C$ 确保我们对所有类别累积代价，从而得到一个可微的整体损失。
换言之，交叉熵在数学上等价于最大化训练数据在预测分布下的对数似然，确保潜在表示能够支持准确的临床分类任务。

**温度缩放的直觉**：
- $\hat{y}_c$ 表示在温度 $T$ 下重新计算的类别概率，分母中的求和依旧覆盖所有 $C$ 个类别以保持概率归一化。
- 将 logits 除以温度 $T$ 会改变 softmax 的曲线形状：$T>1$ 时分布更平滑，$T<1$ 时分布更尖锐。
- 校准数据集 $\mathcal{D}_{\text{val}}$ 提供验证集样本对 $(x, y)$，在求和 $\sum_{(x,y)\in \mathcal{D}_{\text{val}}}$ 时每个样本都会贡献一次对数似然。
- $\mathcal{J}_{\text{cal}}(T)$ 是温度缩放阶段的目标函数，其最小化问题对应寻找最能提升概率校准质量的温度参数。
- $\mathrm{argmin}$ 运算符指出我们要寻找使目标函数达到最小值的温度，从而得到最优标量 $T^*$。
- 在验证集上单独优化 $T$ 能够对模型输出进行后验校准，使置信度数值更贴近实际命中率。例如，当校准后的模型给出“80% 可能阳性”时，应有约 80% 的样本真实为阳性。

### 5.4 总体训练目标
结合生成与监督任务，最终目标（在联合微调阶段）为：

$$
\mathcal{J}(x, y) = \mathcal{L}_{\text{ELBO}}(x) + \lambda \mathcal{L}_{\text{sup}}(x, y) + \gamma \mathcal{R}_{\text{reg}},
$$

其中 $\lambda$ 控制生成与分类的权衡， $\mathcal{R}_{\text{reg}}$ 表示可选的正则项（如权重衰减或对齐约束）， $\gamma$ 为其权重。

**如何理解总体目标？** 该目标函数在联合微调阶段综合了生成、判别与正则化三类需求。
- $\mathcal{L}_{\text{ELBO}}$ 保证潜在空间仍然能够高保真地刻画观测数据分布，是模型的生成基础。
- $\mathcal{L}_{\text{sup}}$ 着力于临床预测的判别性能，引导潜在表示对下游任务保持判别性。
- 正则项 $\mathcal{R}_{\text{reg}}$ 允许纳入权重衰减、潜空间对齐等先验知识，缓解过拟合或保持结构约束。
- 权衡系数 $\lambda$ 和 $\gamma$ 分别缩放监督项与正则项的影响力，从而决定联合训练时哪一部分占主导。这里的 $\lambda$ 就是实践中常说的“分类损失权重”：当 Head 阶段单独训练分类器时，相当于临时将 $\lambda$ 设为极大值；而在联合微调中，我们会把 $\lambda$ 调整回可控范围，让生成与判别目标保持平衡，避免任何一方被完全压制。
- 引入权重系数的设计逻辑在于，生成损失和分类损失往往处于不同的数值尺度，直接相加会导致梯度由其中一项主导。通过 $\lambda$、$\gamma$ 的缩放，研究者可以根据验证集指标（如重建 NLL、AUROC、ECE）选择更偏生成或更偏判别的工作点，从而满足不同临床项目对合成质量和预测可靠性的具体要求。
- 整体损失 $\mathcal{J}(x, y)$ 以样本对 $(x,y)$ 为输入，实际实现中通常对一个批次求平均，以稳定梯度估计。
通过调节权重 $\lambda$ 与 $\gamma$，研究者可以针对具体场景（如更强调可解释生成，或更关注分类可靠性）在多个目标之间取得可控的折衷。


## 6. 应用与展望
- **临床预测**：支持 ICU 死亡率、早期预警等任务，通过校准的概率提升决策透明度。
- **数据共享与隐私保护**：生成合成数据集以便跨机构共享，同时结合成员推断评估监测隐私风险。
- **可解释分析**：通过潜变量分量、互信息、KS/MMD 指标构建可视化报告，帮助医生理解模型行为。

未来计划请参阅 [docs/Roadmap.md](Roadmap.md)。
