{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d2c20b",
   "metadata": {},
   "source": [
    "# MIMIC mortality (unsupervised)\n",
    "\n",
    "This notebook reproduces the unsupervised SUAVE mortality analysis with Optuna-based hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76ae3ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import Dict, List, Mapping, Optional, Tuple\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "EXAMPLES_DIR = Path().resolve()\n",
    "if not EXAMPLES_DIR.exists():\n",
    "    raise RuntimeError(\"Run this notebook from the repository root so 'examples' is available.\")\n",
    "if str(EXAMPLES_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(EXAMPLES_DIR))\n",
    "\n",
    "from mimic_mortality_utils import (\n",
    "    RANDOM_STATE,\n",
    "    TARGET_COLUMNS,\n",
    "    CALIBRATION_SIZE,\n",
    "    VALIDATION_SIZE,\n",
    "    Schema,\n",
    "    define_schema,\n",
    "    SchemaInferenceMode,\n",
    "    compute_auc,\n",
    "    define_schema,\n",
    "    format_float,\n",
    "    kolmogorov_smirnov_statistic,\n",
    "    load_dataset,\n",
    "    mutual_information_feature,\n",
    "    prepare_features,\n",
    "    rbf_mmd,\n",
    "    schema_markdown_table,\n",
    "    split_train_validation_calibration,\n",
    "    to_numeric_frame,\n",
    ")\n",
    "\n",
    "from suave import SUAVE\n",
    "from suave.evaluate import (\n",
    "    evaluate_tstr,\n",
    "    evaluate_trtr,\n",
    "    simple_membership_inference,\n",
    ")\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "except ImportError as exc:  # pragma: no cover - optuna provided via requirements\n",
    "    raise RuntimeError(\n",
    "        \"Optuna is required for the mortality analysis. Install it via 'pip install optuna'.\"\n",
    "    ) from exc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca3cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration\n",
    "analysis_config = {\n",
    "    \"optuna_trials\": 60,\n",
    "    \"optuna_timeout\": 3600*48,\n",
    "    \"optuna_study_prefix\": \"unsupervised\",\n",
    "    \"optuna_storage\": None,\n",
    "    \"output_dir_name\": \"analysis_outputs_unsupervised\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5992f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[schema] Column 'age' flagged for review: Integer feature near categorical threshold.\n",
      "[schema] Column 'sex' flagged for review: Categorical override applied.\n",
      "[schema] Column 'CRRT' flagged for review: Categorical override applied.\n",
      "[schema] Column 'Respiratory_Support' flagged for review: Categorical override applied.\n",
      "[schema] Column 'PaO2' flagged for review: Continuous feature near categorical ratio boundary.\n",
      "[schema] Column 'PaO2/FiO2' flagged for review: Positive skew close to threshold.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Column | Type | n_classes | y_dim |\n",
       "| --- | --- | --- | --- |\n",
       "| age | real |  |  |\n",
       "| sex | cat | 2 |  |\n",
       "| BMI | real |  |  |\n",
       "| temperature | real |  |  |\n",
       "| heart_rate | real |  |  |\n",
       "| respir_rate | real |  |  |\n",
       "| SBP | real |  |  |\n",
       "| DBP | real |  |  |\n",
       "| MAP | real |  |  |\n",
       "| SOFA_cns | ordinal | 5 |  |\n",
       "| CRRT | cat | 2 |  |\n",
       "| Respiratory_Support | ordinal | 5 |  |\n",
       "| WBC | pos |  |  |\n",
       "| Hb | real |  |  |\n",
       "| NE% | real |  |  |\n",
       "| LYM% | real |  |  |\n",
       "| PLT | pos |  |  |\n",
       "| ALT | pos |  |  |\n",
       "| AST | pos |  |  |\n",
       "| STB | pos |  |  |\n",
       "| BUN | pos |  |  |\n",
       "| Scr | pos |  |  |\n",
       "| Glu | pos |  |  |\n",
       "| K+ | real |  |  |\n",
       "| Na+ | real |  |  |\n",
       "| Fg | pos |  |  |\n",
       "| PT | pos |  |  |\n",
       "| APTT | pos |  |  |\n",
       "| PH | real |  |  |\n",
       "| PaO2 | real |  |  |\n",
       "| PaO2/FiO2 | pos |  |  |\n",
       "| PaCO2 | pos |  |  |\n",
       "| HCO3- | real |  |  |\n",
       "| Lac | pos |  |  |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "DATA_DIR = (EXAMPLES_DIR / \"data\" / \"sepsis_mortality_dataset\").resolve()\n",
    "OUTPUT_DIR = EXAMPLES_DIR / analysis_config[\"output_dir_name\"]\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "analysis_config['optuna_storage'] = f'sqlite:///{OUTPUT_DIR}/{analysis_config[\"optuna_study_prefix\"]}_optuna.db'\n",
    "\n",
    "train_df = load_dataset(DATA_DIR / \"mimic-mortality-train.tsv\")\n",
    "test_df = load_dataset(DATA_DIR / \"mimic-mortality-test.tsv\")\n",
    "external_df = load_dataset(DATA_DIR / \"eicu-mortality-external_val.tsv\")\n",
    "\n",
    "FEATURE_COLUMNS = [column for column in train_df.columns if column not in TARGET_COLUMNS]\n",
    "schema = define_schema(train_df, FEATURE_COLUMNS)\n",
    "\n",
    "# manual schema correction\n",
    "schema.update({'BMI':{'type': 'real'}})\n",
    "schema.update({'Respiratory_Support':{'type': 'ordinal', 'n_classes': 5}})\n",
    "schema.update({'LYM%':{'type': 'real'}})\n",
    "\n",
    "schema_table = schema_markdown_table(schema)\n",
    "display(Markdown(schema_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e25b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_latent_classifier() -> Pipeline:\n",
    "    \"\"\"Return the logistic regression pipeline used on latent representations.\"\"\"\n",
    "\n",
    "    return Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"classifier\", LogisticRegression(max_iter=1000)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def make_logistic_pipeline() -> Pipeline:\n",
    "    \"\"\"Factory for the baseline classifier used in TSTR/TRTR.\"\"\"\n",
    "\n",
    "    return Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"classifier\", LogisticRegression(max_iter=200)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def run_optuna_search(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_validation: pd.DataFrame,\n",
    "    y_validation: pd.Series,\n",
    "    schema: Schema,\n",
    "    *,\n",
    "    random_state: int,\n",
    "    n_trials: Optional[int],\n",
    "    timeout: Optional[int],\n",
    "    study_name: Optional[str] = None,\n",
    "    storage: Optional[str] = None,\n",
    ") -> tuple[\"optuna.study.Study\", Dict[str, object]]:\n",
    "    \"\"\"Perform Optuna hyperparameter optimisation for unsupervised :class:`SUAVE`.\"\"\"\n",
    "\n",
    "    hidden_dimension_options: Dict[str, Tuple[int, int]] = {\n",
    "        \"compact\": (128, 64),\n",
    "        \"balanced\": (256, 128),\n",
    "        \"widened\": (384, 192),\n",
    "        \"extended\": (512, 256),\n",
    "    }\n",
    "\n",
    "    if n_trials is not None and n_trials <= 0:\n",
    "        n_trials = None\n",
    "    if timeout is not None and timeout <= 0:\n",
    "        timeout = None\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    def objective(trial: \"optuna.trial.Trial\") -> float:\n",
    "        latent_dim = trial.suggest_categorical(\"latent_dim\", [8, 16, 32, 64, 128])\n",
    "        hidden_key = trial.suggest_categorical(\"hidden_dims\", list(hidden_dimension_options.keys()))\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 5e-5, 2e-2, log=True)\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512, 1024])\n",
    "        beta = trial.suggest_float(\"beta\", 0.25, 4.0)\n",
    "        kl_warmup_epochs = trial.suggest_int(\"kl_warmup_epochs\", 2, 25)\n",
    "        warmup_epochs = trial.suggest_int(\"warmup_epochs\", 10, 60)\n",
    "        n_components = trial.suggest_int(\"n_components\", 1, 8)\n",
    "        tau_start = trial.suggest_float(\"tau_start\", 0.5, 5.0)\n",
    "        tau_min = trial.suggest_float(\"tau_min\", 1e-4, 0.5, log=True)\n",
    "        tau_decay = trial.suggest_float(\"tau_decay\", 1e-4, 0.1, log=True)\n",
    "\n",
    "        model = SUAVE(\n",
    "            schema=schema,\n",
    "            behaviour=\"unsupervised\",\n",
    "            latent_dim=latent_dim,\n",
    "            hidden_dims=hidden_dimension_options[hidden_key],\n",
    "            dropout=dropout,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            beta=beta,\n",
    "            n_components=n_components,\n",
    "            tau_start=tau_start,\n",
    "            tau_min=tau_min,\n",
    "            tau_decay=tau_decay,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            warmup_epochs=warmup_epochs,\n",
    "            kl_warmup_epochs=kl_warmup_epochs,\n",
    "        )\n",
    "        fit_seconds = time.perf_counter() - start_time\n",
    "\n",
    "        latent_classifier = make_latent_classifier()\n",
    "        train_latents = model.encode(X_train)\n",
    "        val_latents = model.encode(X_validation)\n",
    "\n",
    "        if train_latents.size == 0 or val_latents.size == 0:\n",
    "            raise optuna.exceptions.TrialPruned(\"Empty latent representations\")\n",
    "\n",
    "        if np.unique(y_train).size < 2 or np.unique(y_validation).size < 2:\n",
    "            raise optuna.exceptions.TrialPruned(\"Insufficient class diversity\")\n",
    "\n",
    "        latent_classifier.fit(train_latents, np.asarray(y_train))\n",
    "        val_probs = latent_classifier.predict_proba(val_latents)\n",
    "        val_auc = compute_auc(val_probs, y_validation)\n",
    "        if not np.isfinite(val_auc):\n",
    "            raise optuna.exceptions.TrialPruned(\"Non-finite validation AUC\")\n",
    "\n",
    "        numeric_train = to_numeric_frame(X_train)\n",
    "        numeric_val = to_numeric_frame(X_validation)\n",
    "        train_means = numeric_train.mean(axis=0)\n",
    "        train_means = train_means.fillna(0.0)\n",
    "        numeric_train = numeric_train.fillna(train_means)\n",
    "        numeric_val = numeric_val.fillna(train_means)\n",
    "\n",
    "        try:\n",
    "            synthetic_features = model.sample(len(X_train))\n",
    "        except Exception as exc:\n",
    "            raise optuna.exceptions.TrialPruned(f\"Sampling failed: {exc}\") from exc\n",
    "        if not isinstance(synthetic_features, pd.DataFrame):\n",
    "            synthetic_features = pd.DataFrame(synthetic_features, columns=X_train.columns)\n",
    "        synthetic_features = synthetic_features.reindex(columns=X_train.columns)\n",
    "        numeric_synth = to_numeric_frame(synthetic_features).fillna(train_means)\n",
    "\n",
    "        synthetic_latents = model.encode(synthetic_features)\n",
    "        synth_probs = latent_classifier.predict_proba(synthetic_latents)\n",
    "        if synth_probs.ndim == 1:\n",
    "            positive_probs = synth_probs\n",
    "        else:\n",
    "            positive_probs = synth_probs[:, -1]\n",
    "        if not np.all(np.isfinite(positive_probs)):\n",
    "            raise optuna.exceptions.TrialPruned(\"Non-finite synthetic probabilities\")\n",
    "        rng_local = np.random.default_rng(rng.integers(0, 1_000_000))\n",
    "        synthetic_labels = rng_local.binomial(1, np.clip(positive_probs, 1e-4, 1 - 1e-4))\n",
    "        if np.unique(synthetic_labels).size < 2:\n",
    "            raise optuna.exceptions.TrialPruned(\"Synthetic labels lacked class diversity\")\n",
    "\n",
    "        try:\n",
    "            tstr_metrics = evaluate_tstr(\n",
    "                (numeric_synth.to_numpy(), synthetic_labels),\n",
    "                (numeric_val.to_numpy(), y_validation.to_numpy()),\n",
    "                make_logistic_pipeline,\n",
    "            )\n",
    "            trtr_metrics = evaluate_trtr(\n",
    "                (numeric_train.to_numpy(), y_train.to_numpy()),\n",
    "                (numeric_val.to_numpy(), y_validation.to_numpy()),\n",
    "                make_logistic_pipeline,\n",
    "            )\n",
    "        except ValueError as exc:\n",
    "            raise optuna.exceptions.TrialPruned(f\"Classification failed: {exc}\") from exc\n",
    "\n",
    "        tstr_auc = tstr_metrics.get(\"auroc\")\n",
    "        trtr_auc = trtr_metrics.get(\"auroc\")\n",
    "        if not (np.isfinite(tstr_auc) and np.isfinite(trtr_auc)):\n",
    "            raise optuna.exceptions.TrialPruned(\"Non-finite TSTR/TRTR AUC\")\n",
    "\n",
    "        delta_auc = float(tstr_auc - trtr_auc)\n",
    "\n",
    "        trial.set_user_attr(\"validation_auc\", float(val_auc))\n",
    "        trial.set_user_attr(\"fit_seconds\", fit_seconds)\n",
    "        trial.set_user_attr(\n",
    "            \"train_auc\",\n",
    "            compute_auc(latent_classifier.predict_proba(train_latents), y_train),\n",
    "        )\n",
    "        trial.set_user_attr(\"tstr_auc\", float(tstr_auc))\n",
    "        trial.set_user_attr(\"trtr_auc\", float(trtr_auc))\n",
    "        trial.set_user_attr(\"delta_auc\", delta_auc)\n",
    "        return delta_auc\n",
    "\n",
    "    sampler = optuna.samplers.TPESampler(seed=rng.integers(0, 1_000_000))\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=sampler,\n",
    "        study_name=study_name,\n",
    "        storage=storage,\n",
    "        load_if_exists=bool(storage and study_name),\n",
    "    )\n",
    "    study.optimize(objective, n_trials=n_trials, timeout=timeout)\n",
    "\n",
    "    if study.best_trial is None:\n",
    "        raise RuntimeError(\"Optuna search did not produce a best trial\")\n",
    "    best_attributes: Dict[str, object] = {\n",
    "        \"trial_number\": study.best_trial.number,\n",
    "        \"value\": study.best_value,\n",
    "        \"params\": dict(study.best_trial.params),\n",
    "        \"validation_auc\": study.best_trial.user_attrs.get(\"validation_auc\"),\n",
    "        \"fit_seconds\": study.best_trial.user_attrs.get(\"fit_seconds\"),\n",
    "        \"tstr_auc\": study.best_trial.user_attrs.get(\"tstr_auc\"),\n",
    "        \"trtr_auc\": study.best_trial.user_attrs.get(\"trtr_auc\"),\n",
    "        \"delta_auc\": study.best_trial.user_attrs.get(\"delta_auc\", study.best_value),\n",
    "    }\n",
    "    return study, best_attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe95972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training unsupervised model for in_hospital_mortality…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-23 22:35:20,976] A new study created in RDB with name: unsupervised_in_hospital_mortality\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee209b7ca634baf827ef0f16d93099c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised training:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-23 23:39:55,937] Trial 0 finished with value: -0.07268738941326314 and parameters: {'latent_dim': 16, 'hidden_dims': 'balanced', 'dropout': 0.3073187550897951, 'learning_rate': 7.76479325061648e-05, 'batch_size': 64, 'beta': 0.598017013951521, 'kl_warmup_epochs': 21, 'warmup_epochs': 10, 'n_components': 6, 'tau_start': 3.3624624396717526, 'tau_min': 0.07754282945320617, 'tau_decay': 0.009430419398084474}. Best is trial 0 with value: -0.07268738941326314.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41aa78452be842e2809010a8434be598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised training:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "metrics_records: List[Dict[str, object]] = []\n",
    "membership_records: List[Dict[str, object]] = []\n",
    "optuna_reports: Dict[str, Dict[str, object]] = {}\n",
    "\n",
    "latent_models: Dict[str, Pipeline] = {}\n",
    "suave_models: Dict[str, SUAVE] = {}\n",
    "\n",
    "tstr_results: Optional[pd.DataFrame] = None\n",
    "tstr_path: Optional[Path] = None\n",
    "distribution_df: Optional[pd.DataFrame] = None\n",
    "distribution_path: Optional[Path] = None\n",
    "\n",
    "for target in TARGET_COLUMNS:\n",
    "    if target not in train_df.columns:\n",
    "        continue\n",
    "    print(f\"Training unsupervised model for {target}…\")\n",
    "    X_full = prepare_features(train_df, FEATURE_COLUMNS)\n",
    "    y_full = train_df[target]\n",
    "\n",
    "    (\n",
    "        X_train_model,\n",
    "        X_validation,\n",
    "        X_calibration,\n",
    "        y_train_model,\n",
    "        y_validation,\n",
    "        y_calibration,\n",
    "    ) = split_train_validation_calibration(\n",
    "        X_full,\n",
    "        y_full,\n",
    "        calibration_size=CALIBRATION_SIZE,\n",
    "        validation_size=VALIDATION_SIZE,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    X_train_model = pd.concat([X_train_model, X_calibration], ignore_index=True)\n",
    "    y_train_model = pd.concat([y_train_model, y_calibration], ignore_index=True)\n",
    "\n",
    "    study_name = (\n",
    "        f\"{analysis_config['optuna_study_prefix']}_{target}\"\n",
    "        if analysis_config[\"optuna_study_prefix\"]\n",
    "        else None\n",
    "    )\n",
    "    study, best_info = run_optuna_search(\n",
    "        X_train_model,\n",
    "        y_train_model,\n",
    "        X_validation,\n",
    "        y_validation,\n",
    "        schema,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_trials=analysis_config[\"optuna_trials\"],\n",
    "        timeout=analysis_config[\"optuna_timeout\"],\n",
    "        study_name=study_name,\n",
    "        storage=analysis_config[\"optuna_storage\"],\n",
    "    )\n",
    "\n",
    "    hidden_dimension_options: Dict[str, Tuple[int, int]] = {\n",
    "        \"compact\": (128, 64),\n",
    "        \"balanced\": (256, 128),\n",
    "        \"widened\": (384, 192),\n",
    "        \"extended\": (512, 256),\n",
    "    }\n",
    "    best_params = dict(best_info.get(\"params\", {}))\n",
    "    hidden_key = str(best_params.get(\"hidden_dims\", \"balanced\"))\n",
    "    hidden_dims = hidden_dimension_options.get(\n",
    "        hidden_key, hidden_dimension_options[\"balanced\"]\n",
    "    )\n",
    "    model = SUAVE(\n",
    "        schema=schema,\n",
    "        behaviour=\"unsupervised\",\n",
    "        latent_dim=int(best_params.get(\"latent_dim\", 32)),\n",
    "        hidden_dims=hidden_dims,\n",
    "        dropout=float(best_params.get(\"dropout\", 0.1)),\n",
    "        learning_rate=float(best_params.get(\"learning_rate\", 1e-3)),\n",
    "        batch_size=int(best_params.get(\"batch_size\", 256)),\n",
    "        beta=float(best_params.get(\"beta\", 1.5)),\n",
    "        n_components=int(best_params.get(\"n_components\", 1)),\n",
    "        tau_start=float(best_params.get(\"tau_start\", 1.0)),\n",
    "        tau_min=float(best_params.get(\"tau_min\", 0.1)),\n",
    "        tau_decay=float(best_params.get(\"tau_decay\", 0.01)),\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train_model,\n",
    "        warmup_epochs=int(best_params.get(\"warmup_epochs\", 30)),\n",
    "        kl_warmup_epochs=int(best_params.get(\"kl_warmup_epochs\", 10)),\n",
    "    )\n",
    "    suave_models[target] = model\n",
    "\n",
    "    latent_classifier = make_latent_classifier()\n",
    "    train_latents = model.encode(X_train_model)\n",
    "\n",
    "    evaluation_datasets: Dict[str, Tuple[pd.DataFrame, pd.Series]] = {\n",
    "        \"Train\": (X_train_model, y_train_model),\n",
    "        \"Validation\": (X_validation, y_validation),\n",
    "        \"MIMIC test\": (\n",
    "            prepare_features(test_df, FEATURE_COLUMNS),\n",
    "            test_df[target],\n",
    "        ),\n",
    "    }\n",
    "    if target in external_df.columns:\n",
    "        evaluation_datasets[\"eICU external\"] = (\n",
    "            prepare_features(external_df, FEATURE_COLUMNS),\n",
    "            external_df[target],\n",
    "        )\n",
    "\n",
    "    latent_classifier.fit(train_latents, np.asarray(y_train_model))\n",
    "    latent_models[target] = latent_classifier\n",
    "\n",
    "    for dataset_name, (features, labels) in evaluation_datasets.items():\n",
    "        latents = model.encode(features)\n",
    "        probs = latent_classifier.predict_proba(latents)\n",
    "        auc = compute_auc(probs, labels)\n",
    "        metrics_records.append(\n",
    "            {\n",
    "                \"target\": target,\n",
    "                \"dataset\": dataset_name,\n",
    "                \"auc\": auc,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    train_probs = latent_classifier.predict_proba(train_latents)\n",
    "    test_latents = model.encode(evaluation_datasets[\"MIMIC test\"][0])\n",
    "    test_probs = latent_classifier.predict_proba(test_latents)\n",
    "    membership = simple_membership_inference(\n",
    "        train_probs,\n",
    "        np.asarray(y_train_model),\n",
    "        test_probs,\n",
    "        np.asarray(evaluation_datasets[\"MIMIC test\"][1]),\n",
    "    )\n",
    "    membership_records.append({\"target\": target, **membership})\n",
    "\n",
    "    trial_rows: List[Dict[str, object]] = []\n",
    "    for trial in study.trials:\n",
    "        record: Dict[str, object] = {\n",
    "            \"trial_number\": trial.number,\n",
    "            \"value\": trial.value,\n",
    "        }\n",
    "        record.update(trial.params)\n",
    "        validation_auc = trial.user_attrs.get(\"validation_auc\")\n",
    "        if validation_auc is not None:\n",
    "            record[\"validation_auc\"] = validation_auc\n",
    "        fit_seconds = trial.user_attrs.get(\"fit_seconds\")\n",
    "        if fit_seconds is not None:\n",
    "            record[\"fit_seconds\"] = fit_seconds\n",
    "        train_auc = trial.user_attrs.get(\"train_auc\")\n",
    "        if train_auc is not None:\n",
    "            record[\"train_auc\"] = train_auc\n",
    "        tstr_auc = trial.user_attrs.get(\"tstr_auc\")\n",
    "        if tstr_auc is not None:\n",
    "            record[\"tstr_auc\"] = tstr_auc\n",
    "        trtr_auc = trial.user_attrs.get(\"trtr_auc\")\n",
    "        if trtr_auc is not None:\n",
    "            record[\"trtr_auc\"] = trtr_auc\n",
    "        delta_auc = trial.user_attrs.get(\"delta_auc\")\n",
    "        if delta_auc is not None:\n",
    "            record[\"delta_auc\"] = delta_auc\n",
    "        trial_rows.append(record)\n",
    "    trials_df = pd.DataFrame(trial_rows)\n",
    "    trials_path = OUTPUT_DIR / f\"optuna_trials_{target}.csv\"\n",
    "    if not trials_df.empty:\n",
    "        trials_df.to_csv(trials_path, index=False)\n",
    "    else:\n",
    "        trials_path.write_text(\"trial_number,value\\n\")\n",
    "\n",
    "    optuna_reports[target] = {\n",
    "        \"best\": best_info,\n",
    "        \"best_params\": best_params,\n",
    "        \"metrics\": {\n",
    "            row[\"dataset\"]: row[\"auc\"]\n",
    "            for row in metrics_records\n",
    "            if row[\"target\"] == target\n",
    "        },\n",
    "        \"trials_csv\": trials_path,\n",
    "    }\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_records)\n",
    "metrics_path = OUTPUT_DIR / \"evaluation_metrics_unsupervised.csv\"\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "\n",
    "membership_df = pd.DataFrame(membership_records)\n",
    "membership_path = OUTPUT_DIR / \"membership_inference_unsupervised.csv\"\n",
    "membership_df.to_csv(membership_path, index=False)\n",
    "\n",
    "primary_target = \"in_hospital_mortality\"\n",
    "if primary_target in suave_models and primary_target in latent_models:\n",
    "    print(\"Generating synthetic data for TSTR/TRTR comparisons…\")\n",
    "    model = suave_models[primary_target]\n",
    "    latent_classifier = latent_models[primary_target]\n",
    "\n",
    "    X_train_full = prepare_features(train_df, FEATURE_COLUMNS)\n",
    "    y_train_full = train_df[primary_target]\n",
    "    numeric_train = to_numeric_frame(X_train_full)\n",
    "    train_means = numeric_train.mean(axis=0)\n",
    "    train_means = train_means.fillna(0.0)\n",
    "    numeric_train = numeric_train.fillna(train_means)\n",
    "\n",
    "    synthetic_features = model.sample(len(X_train_full))\n",
    "    synthetic_features = synthetic_features[FEATURE_COLUMNS]\n",
    "    numeric_synthetic = to_numeric_frame(synthetic_features)\n",
    "    numeric_synthetic = numeric_synthetic.fillna(train_means)\n",
    "\n",
    "    synthetic_latents = model.encode(synthetic_features)\n",
    "    synthetic_probs = latent_classifier.predict_proba(synthetic_latents)[:, 1]\n",
    "    rng = np.random.default_rng(RANDOM_STATE)\n",
    "    synthetic_labels = rng.binomial(1, synthetic_probs)\n",
    "\n",
    "    numeric_test = to_numeric_frame(prepare_features(test_df, FEATURE_COLUMNS))\n",
    "    numeric_test = numeric_test.fillna(train_means)\n",
    "    y_test = test_df[primary_target]\n",
    "\n",
    "    tstr_metrics = evaluate_tstr(\n",
    "        (numeric_synthetic.to_numpy(), synthetic_labels),\n",
    "        (numeric_test.to_numpy(), y_test.to_numpy()),\n",
    "        make_logistic_pipeline,\n",
    "    )\n",
    "    trtr_metrics = evaluate_trtr(\n",
    "        (numeric_train.to_numpy(), y_train_full.to_numpy()),\n",
    "        (numeric_test.to_numpy(), y_test.to_numpy()),\n",
    "        make_logistic_pipeline,\n",
    "    )\n",
    "    tstr_results = pd.DataFrame(\n",
    "        [\n",
    "            {\"setting\": \"TSTR\", **tstr_metrics},\n",
    "            {\"setting\": \"TRTR\", **trtr_metrics},\n",
    "        ]\n",
    "    )\n",
    "    tstr_path = OUTPUT_DIR / \"tstr_trtr_comparison_unsupervised.csv\"\n",
    "    tstr_results.to_csv(tstr_path, index=False)\n",
    "\n",
    "    distribution_rows: List[Dict[str, object]] = []\n",
    "    for column in FEATURE_COLUMNS:\n",
    "        real_values = numeric_train[column].to_numpy()\n",
    "        synthetic_values = numeric_synthetic[column].to_numpy()\n",
    "        distribution_rows.append(\n",
    "            {\n",
    "                \"feature\": column,\n",
    "                \"ks\": kolmogorov_smirnov_statistic(real_values, synthetic_values),\n",
    "                \"mmd\": rbf_mmd(\n",
    "                    real_values, synthetic_values, random_state=RANDOM_STATE\n",
    "                ),\n",
    "                \"mutual_information\": mutual_information_feature(\n",
    "                    real_values, synthetic_values\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "    distribution_df = pd.DataFrame(distribution_rows)\n",
    "    distribution_path = OUTPUT_DIR / \"distribution_shift_metrics_unsupervised.csv\"\n",
    "    distribution_df.to_csv(distribution_path, index=False)\n",
    "else:\n",
    "    print(\"Primary target model not available; skipping TSTR/TRTR and distribution analysis.\")\n",
    "\n",
    "\n",
    "summary_lines: List[str] = [\n",
    "    \"# Unsupervised mortality modelling report\",\n",
    "    \"\",\n",
    "    \"## Schema\",\n",
    "    schema_table,\n",
    "    \"\",\n",
    "    \"## Model selection and performance\",\n",
    "]\n",
    "\n",
    "if not optuna_reports:\n",
    "    summary_lines.append(\"No models were trained.\")\n",
    "\n",
    "for target, report in optuna_reports.items():\n",
    "    best = report[\"best\"]\n",
    "    best_params = report[\"best_params\"]\n",
    "    metrics_map: Mapping[str, float] = report[\"metrics\"]\n",
    "    summary_lines.append(f\"### {target}\")\n",
    "    best_value = best.get(\"value\")\n",
    "    value_text = (\n",
    "        f\"{best_value:.4f}\" if isinstance(best_value, (int, float)) else \"n/a\"\n",
    "    )\n",
    "    summary_lines.append(\n",
    "        f\"Best Optuna trial #{best.get('trial_number')} with delta AUC (TSTR-TRTR) {value_text}\"\n",
    "    )\n",
    "    summary_lines.append(\"Best parameters:\")\n",
    "    summary_lines.append(\"```json\")\n",
    "    summary_lines.append(json.dumps(best_params, indent=2, ensure_ascii=False))\n",
    "    summary_lines.append(\"```\")\n",
    "    if best.get(\"tstr_auc\") is not None and best.get(\"trtr_auc\") is not None:\n",
    "        summary_lines.append(\n",
    "            \"TSTR AUC: {tstr} | TRTR AUC: {trtr} | Delta: {delta}\".format(\n",
    "                tstr=format_float(best.get(\"tstr_auc\")),\n",
    "                trtr=format_float(best.get(\"trtr_auc\")),\n",
    "                delta=format_float(best.get(\"delta_auc\")),\n",
    "            )\n",
    "        )\n",
    "    summary_lines.append(\"| Dataset | AUC |\")\n",
    "    summary_lines.append(\"| --- | --- |\")\n",
    "    for dataset_name in [\n",
    "        \"Train\",\n",
    "        \"Validation\",\n",
    "        \"MIMIC test\",\n",
    "        \"eICU external\",\n",
    "    ]:\n",
    "        if dataset_name not in metrics_map:\n",
    "            continue\n",
    "        summary_lines.append(\n",
    "            \"| {dataset} | {auc} |\".format(\n",
    "                dataset=dataset_name,\n",
    "                auc=format_float(metrics_map.get(dataset_name)),\n",
    "            )\n",
    "        )\n",
    "    summary_lines.append(\n",
    "        f\"Optuna trials logged at: {report['trials_csv'].relative_to(OUTPUT_DIR)}\"\n",
    "    )\n",
    "    summary_lines.append(\"\")\n",
    "\n",
    "if tstr_results is not None:\n",
    "    summary_lines.append(\"## TSTR vs TRTR\")\n",
    "    summary_lines.append(\"| Setting | Accuracy | AUC | AUPRC | Brier | ECE |\")\n",
    "    summary_lines.append(\"| --- | --- | --- | --- | --- | --- |\")\n",
    "    for _, row in tstr_results.iterrows():\n",
    "        summary_lines.append(\n",
    "            \"| {setting} | {acc:.3f} | {auc:.3f} | {auprc:.3f} | {brier:.3f} | {ece:.3f} |\".format(\n",
    "                setting=row[\"setting\"],\n",
    "                acc=row.get(\"accuracy\", np.nan),\n",
    "                auc=row.get(\"auroc\", np.nan),\n",
    "                auprc=row.get(\"auprc\", np.nan),\n",
    "                brier=row.get(\"brier\", np.nan),\n",
    "                ece=row.get(\"ece\", np.nan),\n",
    "            )\n",
    "        )\n",
    "    summary_lines.append(\"\")\n",
    "\n",
    "summary_lines.append(\"## Distribution shift and privacy\")\n",
    "if distribution_df is not None and distribution_path is not None:\n",
    "    distribution_top = distribution_df.sort_values(\"ks\", ascending=False).head(10)\n",
    "    summary_lines.append(\"Top 10 features by KS statistic:\")\n",
    "    summary_lines.append(\"| Feature | KS | MMD | Mutual information |\")\n",
    "    summary_lines.append(\"| --- | --- | --- | --- |\")\n",
    "    for _, row in distribution_top.iterrows():\n",
    "        summary_lines.append(\n",
    "            \"| {feature} | {ks:.3f} | {mmd:.3f} | {mi:.3f} |\".format(\n",
    "                feature=row[\"feature\"],\n",
    "                ks=row.get(\"ks\", np.nan),\n",
    "                mmd=row.get(\"mmd\", np.nan),\n",
    "                mi=row.get(\"mutual_information\", np.nan),\n",
    "            )\n",
    "        )\n",
    "    summary_lines.append(\n",
    "        f\"Full distribution metrics: {distribution_path.relative_to(OUTPUT_DIR)}\"\n",
    "    )\n",
    "else:\n",
    "    summary_lines.append(\"Distribution metrics were not computed.\")\n",
    "\n",
    "if not membership_records:\n",
    "    summary_lines.append(\"No membership inference metrics were recorded.\")\n",
    "else:\n",
    "    summary_lines.append(\"Membership inference results:\")\n",
    "    summary_lines.append(\n",
    "        \"| Target | attack_auc | attack_accuracy | attack_threshold |\"\n",
    "    )\n",
    "    summary_lines.append(\"| --- | --- | --- | --- |\")\n",
    "    for _, row in pd.DataFrame(membership_records).iterrows():\n",
    "        summary_lines.append(\n",
    "            \"| {target} | {auc:.3f} | {accuracy:.3f} | {threshold:.3f} |\".format(\n",
    "                target=row[\"target\"],\n",
    "                auc=row.get(\"attack_auc\", np.nan),\n",
    "                accuracy=row.get(\"attack_best_accuracy\", np.nan),\n",
    "                threshold=row.get(\"attack_best_threshold\", np.nan),\n",
    "            )\n",
    "        )\n",
    "    summary_lines.append(\n",
    "        f\"Membership metrics saved to: {membership_path.relative_to(OUTPUT_DIR)}\"\n",
    "    )\n",
    "\n",
    "summary_path = OUTPUT_DIR / \"summary_unsupervised.md\"\n",
    "summary_path.write_text(\"\\n\".join(summary_lines), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Analysis complete.\")\n",
    "print(f\"Metric table saved to {metrics_path}\")\n",
    "print(f\"Membership inference results saved to {membership_path}\")\n",
    "if tstr_path is not None and distribution_path is not None:\n",
    "    print(f\"TSTR/TRTR comparison saved to {tstr_path}\")\n",
    "    print(f\"Distribution metrics saved to {distribution_path}\")\n",
    "print(f\"Summary written to {summary_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dba732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}