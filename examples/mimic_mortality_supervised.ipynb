{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b241d7fb",
   "metadata": {},
   "source": [
    "# MIMIC mortality (supervised)\n",
    "\n",
    "This notebook reproduces the supervised SUAVE mortality analysis with Optuna-based hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aea563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import Dict, List, Mapping, Optional, Tuple\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    brier_score_loss,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "EXAMPLES_DIR = Path().resolve()\n",
    "if not EXAMPLES_DIR.exists():\n",
    "    raise RuntimeError(\"Run this notebook from the repository root so 'examples' is available.\")\n",
    "if str(EXAMPLES_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(EXAMPLES_DIR))\n",
    "\n",
    "from mimic_mortality_utils import (\n",
    "    RANDOM_STATE,\n",
    "    TARGET_COLUMNS,\n",
    "    CALIBRATION_SIZE,\n",
    "    VALIDATION_SIZE,\n",
    "    Schema,\n",
    "    define_schema,\n",
    "    \n",
    "    format_float,\n",
    "    kolmogorov_smirnov_statistic,\n",
    "    load_dataset,\n",
    "    mutual_information_feature,\n",
    "    prepare_features,\n",
    "    rbf_mmd,\n",
    "    schema_markdown_table,\n",
    "    split_train_validation_calibration,\n",
    "    to_numeric_frame,\n",
    ")\n",
    "\n",
    "from suave import SUAVE\n",
    "from suave.evaluate import evaluate_tstr, evaluate_trtr, simple_membership_inference\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "except ImportError as exc:  # pragma: no cover - optuna provided via requirements\n",
    "    raise RuntimeError(\n",
    "        \"Optuna is required for the mortality analysis. Install it via 'pip install optuna'.\"\n",
    "    ) from exc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "085afd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration\n",
    "analysis_config = {\n",
    "    \"optuna_trials\": 60,\n",
    "    \"optuna_timeout\": 3600*48,\n",
    "    \"optuna_study_prefix\": \"supervised\",\n",
    "    \"optuna_storage\": None,\n",
    "    \"output_dir_name\": \"analysis_outputs_supervised\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe95aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[schema] Column 'age' flagged for review: Integer feature near categorical threshold.\n",
      "[schema] Column 'PaO2' flagged for review: Continuous feature near categorical ratio boundary.\n",
      "[schema] Column 'PaO2/FiO2' flagged for review: Positive skew close to threshold.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Column | Type | n_classes | y_dim |\n",
       "| --- | --- | --- | --- |\n",
       "| age | real |  |  |\n",
       "| sex | cat | 2 |  |\n",
       "| BMI | real |  |  |\n",
       "| temperature | real |  |  |\n",
       "| heart_rate | real |  |  |\n",
       "| respir_rate | real |  |  |\n",
       "| SBP | real |  |  |\n",
       "| DBP | real |  |  |\n",
       "| MAP | real |  |  |\n",
       "| SOFA_cns | ordinal | 5 |  |\n",
       "| CRRT | cat | 2 |  |\n",
       "| Respiratory_Support | ordinal | 5 |  |\n",
       "| WBC | pos |  |  |\n",
       "| Hb | real |  |  |\n",
       "| NE% | real |  |  |\n",
       "| LYM% | real |  |  |\n",
       "| PLT | pos |  |  |\n",
       "| ALT | pos |  |  |\n",
       "| AST | pos |  |  |\n",
       "| STB | pos |  |  |\n",
       "| BUN | pos |  |  |\n",
       "| Scr | pos |  |  |\n",
       "| Glu | pos |  |  |\n",
       "| K+ | real |  |  |\n",
       "| Na+ | real |  |  |\n",
       "| Fg | pos |  |  |\n",
       "| PT | pos |  |  |\n",
       "| APTT | pos |  |  |\n",
       "| PH | real |  |  |\n",
       "| PaO2 | real |  |  |\n",
       "| PaO2/FiO2 | pos |  |  |\n",
       "| PaCO2 | pos |  |  |\n",
       "| HCO3- | real |  |  |\n",
       "| Lac | pos |  |  |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "DATA_DIR = (EXAMPLES_DIR / \"data\" / \"sepsis_mortality_dataset\").resolve()\n",
    "OUTPUT_DIR = EXAMPLES_DIR / analysis_config[\"output_dir_name\"]\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "analysis_config['optuna_storage'] = f'sqlite:///{OUTPUT_DIR}/{analysis_config[\"optuna_study_prefix\"]}_optuna.db'\n",
    "\n",
    "train_df = load_dataset(DATA_DIR / \"mimic-mortality-train.tsv\")\n",
    "test_df = load_dataset(DATA_DIR / \"mimic-mortality-test.tsv\")\n",
    "external_df = load_dataset(DATA_DIR / \"eicu-mortality-external_val.tsv\")\n",
    "\n",
    "FEATURE_COLUMNS = [column for column in train_df.columns if column not in TARGET_COLUMNS]\n",
    "schema = define_schema(train_df, FEATURE_COLUMNS, mode=\"interactive\")\n",
    "\n",
    "# manual schema correction\n",
    "schema.update({'BMI':{'type': 'real'},\n",
    "               'Respiratory_Support':{'type': 'ordinal', 'n_classes': 5},\n",
    "               'LYM%':{'type': 'real'}\n",
    "               })\n",
    "\n",
    "\n",
    "schema_table = schema_markdown_table(schema)\n",
    "display(Markdown(schema_table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bd362ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HIDDEN_DIMENSION_OPTIONS: Dict[str, Tuple[int, int]] = {\n",
    "    \"compact\": (96, 48),\n",
    "    \"small\": (128, 64),\n",
    "    \"medium\": (256, 128),\n",
    "    \"wide\": (384, 192),\n",
    "    \"extra_wide\": (512, 256),\n",
    "}\n",
    "\n",
    "HEAD_HIDDEN_DIMENSION_OPTIONS: Dict[str, Tuple[int, int]] = {\n",
    "    \"compact\": (32,),\n",
    "    \"small\": (48,),\n",
    "    \"medium\": (48, 32),\n",
    "    \"wide\": (96, 48, 16),\n",
    "    \"extra_wide\": (64, 128, 64, 16),\n",
    "}\n",
    "\n",
    "\n",
    "def make_logistic_pipeline() -> Pipeline:\n",
    "    \"\"\"Factory for the baseline classifier used in TSTR/TRTR.\"\"\"\n",
    "\n",
    "    return Pipeline(\n",
    "        [\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"classifier\", LogisticRegression(max_iter=200)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_binary_metrics(\n",
    "    probabilities: np.ndarray, targets: pd.Series | np.ndarray\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Compute AUROC, accuracy, specificity, sensitivity, and Brier score.\"\"\"\n",
    "\n",
    "    prob_matrix = np.asarray(probabilities)\n",
    "    if prob_matrix.ndim == 1:\n",
    "        positive_probs = prob_matrix\n",
    "    else:\n",
    "        positive_probs = prob_matrix[:, -1]\n",
    "    labels = np.asarray(targets)\n",
    "    predictions = (positive_probs >= 0.5).astype(int)\n",
    "\n",
    "    metrics: Dict[str, float] = {}\n",
    "\n",
    "    try:\n",
    "        roauc = float(roc_auc_score(labels, positive_probs))\n",
    "    except ValueError:\n",
    "        roauc = float(\"nan\")\n",
    "\n",
    "    metrics[\"ROAUC\"] = roauc\n",
    "    metrics[\"AUC\"] = roauc\n",
    "\n",
    "    metrics[\"ACC\"] = float(accuracy_score(labels, predictions))\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, predictions, labels=[0, 1]).ravel()\n",
    "    metrics[\"SPE\"] = float(tn / (tn + fp)) if (tn + fp) > 0 else float(\"nan\")\n",
    "    metrics[\"SEN\"] = float(tp / (tp + fn)) if (tp + fn) > 0 else float(\"nan\")\n",
    "    metrics[\"Brier\"] = float(brier_score_loss(labels, positive_probs))\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def run_optuna_search(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_validation: pd.DataFrame,\n",
    "    y_validation: pd.Series,\n",
    "    schema: Schema,\n",
    "    *,\n",
    "    random_state: int,\n",
    "    n_trials: Optional[int],\n",
    "    timeout: Optional[int],\n",
    "    study_name: Optional[str] = None,\n",
    "    storage: Optional[str] = None,\n",
    ") -> tuple[\"optuna.study.Study\", Dict[str, object]]:\n",
    "    \"\"\"Perform Optuna hyperparameter optimisation for :class:`SUAVE`.\"\"\"\n",
    "\n",
    "    if n_trials is not None and n_trials <= 0:\n",
    "        n_trials = None\n",
    "    if timeout is not None and timeout <= 0:\n",
    "        timeout = None\n",
    "\n",
    "    def objective(trial: \"optuna.trial.Trial\") -> float:\n",
    "        latent_dim = trial.suggest_categorical(\"latent_dim\", [6, 8, 16, 32])\n",
    "        hidden_key = trial.suggest_categorical(\"hidden_dims\", list(HIDDEN_DIMENSION_OPTIONS.keys()))\n",
    "        head_hidden_key = trial.suggest_categorical(\"head_hidden_dims\", list(HEAD_HIDDEN_DIMENSION_OPTIONS.keys()))\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 5e-5, 2e-2, log=True)\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512, 1024])\n",
    "        beta = trial.suggest_float(\"beta\", 0.5, 4.0)\n",
    "        warmup_epochs = trial.suggest_int(\"warmup_epochs\", 2, 60)\n",
    "        head_epochs = trial.suggest_int(\"head_epochs\", 1, 50)\n",
    "        finetune_epochs = trial.suggest_int(\"finetune_epochs\", 3, 20)\n",
    "        joint_decoder_lr_scale = trial.suggest_float(\"joint_decoder_lr_scale\", 1e-3, 0.3)\n",
    "\n",
    "        model = SUAVE(\n",
    "            schema=schema,\n",
    "            latent_dim=latent_dim,\n",
    "            hidden_dims=HIDDEN_DIMENSION_OPTIONS[hidden_key],\n",
    "            head_hidden_dims=HEAD_HIDDEN_DIMENSION_OPTIONS[head_hidden_key],\n",
    "            dropout=dropout,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            beta=beta,\n",
    "            random_state=random_state,\n",
    "            behaviour=\"supervised\",\n",
    "        )\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            warmup_epochs=warmup_epochs,\n",
    "            head_epochs=head_epochs,\n",
    "            finetune_epochs=finetune_epochs,\n",
    "            joint_decoder_lr_scale=joint_decoder_lr_scale,\n",
    "        )\n",
    "        fit_seconds = time.perf_counter() - start_time\n",
    "        validation_probs = model.predict_proba(X_validation)\n",
    "        validation_metrics = compute_binary_metrics(validation_probs, y_validation)\n",
    "        trial.set_user_attr(\"validation_metrics\", validation_metrics)\n",
    "        trial.set_user_attr(\"fit_seconds\", fit_seconds)\n",
    "\n",
    "        roauc = validation_metrics.get(\"ROAUC\", float(\"nan\"))\n",
    "        if not np.isfinite(roauc):\n",
    "            raise optuna.exceptions.TrialPruned(\"Non-finite validation ROAUC\")\n",
    "        return roauc\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=study_name,\n",
    "        storage=storage,\n",
    "        load_if_exists=bool(storage and study_name),\n",
    "    )\n",
    "    study.optimize(objective, n_trials=n_trials, timeout=timeout)\n",
    "\n",
    "    if study.best_trial is None:\n",
    "        raise RuntimeError(\"Optuna search did not produce a best trial\")\n",
    "    best_attributes: Dict[str, object] = {\n",
    "        \"trial_number\": study.best_trial.number,\n",
    "        \"value\": study.best_value,\n",
    "        \"params\": dict(study.best_trial.params),\n",
    "        \"validation_metrics\": study.best_trial.user_attrs.get(\"validation_metrics\", {}),\n",
    "        \"fit_seconds\": study.best_trial.user_attrs.get(\"fit_seconds\"),\n",
    "    }\n",
    "    return study, best_attributes\n",
    "\n",
    "\n",
    "def plot_calibration_curves(\n",
    "    probability_map: Mapping[str, np.ndarray],\n",
    "    label_map: Mapping[str, np.ndarray],\n",
    "    *,\n",
    "    target_name: str,\n",
    "    output_path: Path,\n",
    "    n_bins: int = 10,\n",
    ") -> None:\n",
    "    \"\"\"Generate calibration curves with Brier scores annotated in the legend.\"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    ax.plot(\n",
    "        [0, 1], [0, 1], linestyle=\"--\", color=\"tab:gray\", label=\"Perfect calibration\"\n",
    "    )\n",
    "\n",
    "    for dataset_name, probs in probability_map.items():\n",
    "        labels = label_map[dataset_name]\n",
    "        if probs.ndim == 2:\n",
    "            pos_probs = probs[:, -1]\n",
    "        else:\n",
    "            pos_probs = probs\n",
    "        try:\n",
    "            frac_pos, mean_pred = calibration_curve(labels, pos_probs, n_bins=n_bins)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        brier = brier_score_loss(labels, pos_probs)\n",
    "        ax.plot(\n",
    "            mean_pred, frac_pos, marker=\"o\", label=f\"{dataset_name} (Brier={brier:.3f})\"\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Predicted probability\")\n",
    "    ax.set_ylabel(\"Observed frequency\")\n",
    "    ax.set_title(f\"Calibration: {target_name}\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(output_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_latent_space(\n",
    "    model: SUAVE,\n",
    "    feature_map: Mapping[str, pd.DataFrame],\n",
    "    label_map: Mapping[str, pd.Series | np.ndarray],\n",
    "    *,\n",
    "    target_name: str,\n",
    "    output_path: Path,\n",
    ") -> None:\n",
    "    \"\"\"Project latent representations with PCA and create scatter plots.\"\"\"\n",
    "\n",
    "    latent_blocks: List[np.ndarray] = []\n",
    "    dataset_keys: List[str] = []\n",
    "    for name, features in feature_map.items():\n",
    "        if features.empty:\n",
    "            continue\n",
    "        latents = model.encode(features)\n",
    "        if latents.size == 0:\n",
    "            continue\n",
    "        latent_blocks.append(latents)\n",
    "        dataset_keys.append(name)\n",
    "\n",
    "    if not latent_blocks:\n",
    "        return\n",
    "\n",
    "    concatenated = np.vstack(latent_blocks)\n",
    "    pca = PCA(n_components=2)\n",
    "    projected = pca.fit_transform(concatenated)\n",
    "\n",
    "    offsets = np.cumsum([0] + [block.shape[0] for block in latent_blocks])\n",
    "    fig, axes = plt.subplots(\n",
    "        1,\n",
    "        len(latent_blocks),\n",
    "        figsize=(6 * len(latent_blocks), 5),\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "    )\n",
    "\n",
    "    if len(latent_blocks) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx, (ax, name) in enumerate(zip(axes, dataset_keys)):\n",
    "        start, end = offsets[idx], offsets[idx + 1]\n",
    "        subset = projected[start:end]\n",
    "        labels = np.asarray(label_map[name])\n",
    "        scatter = ax.scatter(\n",
    "            subset[:, 0],\n",
    "            subset[:, 1],\n",
    "            c=labels,\n",
    "            cmap=\"coolwarm\",\n",
    "            alpha=0.7,\n",
    "            edgecolor=\"none\",\n",
    "        )\n",
    "        ax.set_title(f\"{name}\")\n",
    "        ax.set_xlabel(\"PC1\")\n",
    "        ax.set_ylabel(\"PC2\")\n",
    "        legend = ax.legend(*scatter.legend_elements(), title=\"Label\")\n",
    "        ax.add_artist(legend)\n",
    "\n",
    "    fig.suptitle(f\"Latent space projection: {target_name}\")\n",
    "    fig.tight_layout(rect=(0, 0, 1, 0.96))\n",
    "    fig.savefig(output_path, dpi=300)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c44d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-24 02:30:40,554] Using an existing study with name 'supervised_in_hospital_mortality' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training supervised model for in_hospital_mortality\u2026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b0f6c8ee44412b8e628f8864712390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Warm-start:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1271af6b3e824142ac448fceb971aa4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Head:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3ebaa985da4f52ab10911a18aad605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Joint fine-tune:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-24 02:31:27,538] Trial 1 finished with value: 0.5669088997312344 and parameters: {'latent_dim': 32, 'hidden_dims': 'compact', 'head_hidden_dims': 'wide', 'dropout': 0.30186623384978056, 'learning_rate': 5.313609832071856e-05, 'batch_size': 1024, 'beta': 1.6953873312905763, 'warmup_epochs': 11, 'head_epochs': 35, 'finetune_epochs': 10, 'joint_decoder_lr_scale': 0.07995769511797722}. Best is trial 1 with value: 0.5669088997312344.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63146fed89ce4061b8b8dad87be1e367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Warm-start:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7332d8c8048a4acca270a9fa74763694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Head:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics_records: List[Dict[str, object]] = []\n",
    "membership_records: List[Dict[str, object]] = []\n",
    "optuna_reports: Dict[str, Dict[str, object]] = {}\n",
    "calibration_paths: Dict[str, Path] = {}\n",
    "latent_paths: Dict[str, Path] = {}\n",
    "\n",
    "models: Dict[str, SUAVE] = {}\n",
    "\n",
    "tstr_results: Optional[pd.DataFrame] = None\n",
    "tstr_path: Optional[Path] = None\n",
    "distribution_df: Optional[pd.DataFrame] = None\n",
    "distribution_path: Optional[Path] = None\n",
    "\n",
    "for target in TARGET_COLUMNS:\n",
    "    if target not in train_df.columns:\n",
    "        continue\n",
    "    print(f\"Training supervised model for {target}\u2026\")\n",
    "    X_full = prepare_features(train_df, FEATURE_COLUMNS)\n",
    "    y_full = train_df[target]\n",
    "\n",
    "    (\n",
    "        X_train_model,\n",
    "        X_validation,\n",
    "        X_calibration,\n",
    "        y_train_model,\n",
    "        y_validation,\n",
    "        y_calibration,\n",
    "    ) = split_train_validation_calibration(\n",
    "        X_full,\n",
    "        y_full,\n",
    "        calibration_size=CALIBRATION_SIZE,\n",
    "        validation_size=VALIDATION_SIZE,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    study_name = (\n",
    "        f\"{analysis_config['optuna_study_prefix']}_{target}\"\n",
    "        if analysis_config[\"optuna_study_prefix\"]\n",
    "        else None\n",
    "    )\n",
    "    study, best_info = run_optuna_search(\n",
    "        X_train_model,\n",
    "        y_train_model,\n",
    "        X_validation,\n",
    "        y_validation,\n",
    "        schema,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_trials=analysis_config[\"optuna_trials\"],\n",
    "        timeout=analysis_config[\"optuna_timeout\"],\n",
    "        study_name=study_name,\n",
    "        storage=analysis_config[\"optuna_storage\"],\n",
    "    )\n",
    "\n",
    "    best_params = dict(best_info.get(\"params\", {}))\n",
    "    hidden_key = str(best_params.get(\"hidden_dims\", \"medium\"))\n",
    "    head_hidden_key = str(best_params.get(\"head_hidden_dims\", \"medium\"))\n",
    "    hidden_dims = HIDDEN_DIMENSION_OPTIONS.get(\n",
    "        hidden_key, HIDDEN_DIMENSION_OPTIONS[\"medium\"]\n",
    "    )\n",
    "    head_hidden_dims = HEAD_HIDDEN_DIMENSION_OPTIONS.get(\n",
    "        head_hidden_key, HEAD_HIDDEN_DIMENSION_OPTIONS[\"medium\"]\n",
    "    )\n",
    "    model = SUAVE(\n",
    "        schema=schema,\n",
    "        latent_dim=int(best_params.get(\"latent_dim\", 16)),\n",
    "        hidden_dims=hidden_dims,\n",
    "        head_hidden_dims=head_hidden_dims,\n",
    "        dropout=float(best_params.get(\"dropout\", 0.1)),\n",
    "        learning_rate=float(best_params.get(\"learning_rate\", 1e-3)),\n",
    "        batch_size=int(best_params.get(\"batch_size\", 256)),\n",
    "        beta=float(best_params.get(\"beta\", 1.5)),\n",
    "        random_state=RANDOM_STATE,\n",
    "        behaviour=\"supervised\",\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train_model,\n",
    "        y_train_model,\n",
    "        warmup_epochs=int(best_params.get(\"warmup_epochs\", 3)),\n",
    "        head_epochs=int(best_params.get(\"head_epochs\", 2)),\n",
    "        finetune_epochs=int(best_params.get(\"finetune_epochs\", 2)),\n",
    "        joint_decoder_lr_scale=float(best_params.get(\"joint_decoder_lr_scale\", 0.1)),\n",
    "    )\n",
    "    model.calibrate(X_calibration, y_calibration)\n",
    "    models[target] = model\n",
    "\n",
    "    evaluation_datasets: Dict[str, Tuple[pd.DataFrame, pd.Series]] = {\n",
    "        \"Train\": (X_train_model, y_train_model),\n",
    "        \"Validation\": (X_validation, y_validation),\n",
    "        \"MIMIC test\": (prepare_features(test_df, FEATURE_COLUMNS), test_df[target]),\n",
    "    }\n",
    "    if target in external_df.columns:\n",
    "        evaluation_datasets[\"eICU external\"] = (\n",
    "            prepare_features(external_df, FEATURE_COLUMNS),\n",
    "            external_df[target],\n",
    "        )\n",
    "\n",
    "    probability_map: Dict[str, np.ndarray] = {}\n",
    "    label_map: Dict[str, np.ndarray] = {}\n",
    "    dataset_metric_map: Dict[str, Dict[str, float]] = {}\n",
    "\n",
    "    for dataset_name, (features, labels) in evaluation_datasets.items():\n",
    "        probs = model.predict_proba(features)\n",
    "        probability_map[dataset_name] = probs\n",
    "        label_map[dataset_name] = np.asarray(labels)\n",
    "        metrics = compute_binary_metrics(probs, labels)\n",
    "        dataset_metric_map[dataset_name] = metrics\n",
    "        metric_row = {\n",
    "            \"target\": target,\n",
    "            \"dataset\": dataset_name,\n",
    "            **metrics,\n",
    "        }\n",
    "        metrics_records.append(metric_row)\n",
    "\n",
    "    calibration_path = OUTPUT_DIR / f\"calibration_{target}.png\"\n",
    "    plot_calibration_curves(\n",
    "        probability_map, label_map, target_name=target, output_path=calibration_path\n",
    "    )\n",
    "    calibration_paths[target] = calibration_path\n",
    "\n",
    "    latent_features = {\n",
    "        name: features for name, (features, _) in evaluation_datasets.items()\n",
    "    }\n",
    "    latent_labels = {\n",
    "        name: labels for name, (_, labels) in evaluation_datasets.items()\n",
    "    }\n",
    "    latent_path = OUTPUT_DIR / f\"latent_{target}.png\"\n",
    "    plot_latent_space(\n",
    "        model,\n",
    "        latent_features,\n",
    "        latent_labels,\n",
    "        target_name=target,\n",
    "        output_path=latent_path,\n",
    "    )\n",
    "    latent_paths[target] = latent_path\n",
    "\n",
    "    train_probabilities = probability_map[\"Train\"]\n",
    "    test_probabilities = probability_map[\"MIMIC test\"]\n",
    "\n",
    "    membership = simple_membership_inference(\n",
    "        train_probabilities,\n",
    "        np.asarray(y_train_model),\n",
    "        test_probabilities,\n",
    "        np.asarray(evaluation_datasets[\"MIMIC test\"][1]),\n",
    "    )\n",
    "    membership_records.append({\"target\": target, **membership})\n",
    "\n",
    "    trial_rows: List[Dict[str, object]] = []\n",
    "    for trial in study.trials:\n",
    "        record: Dict[str, object] = {\n",
    "            \"trial_number\": trial.number,\n",
    "            \"value\": trial.value,\n",
    "        }\n",
    "        record.update(trial.params)\n",
    "        val_metrics = trial.user_attrs.get(\"validation_metrics\")\n",
    "        if isinstance(val_metrics, Mapping):\n",
    "            for metric_name, metric_value in val_metrics.items():\n",
    "                record[f\"validation_{metric_name.lower()}\"] = metric_value\n",
    "        fit_seconds = trial.user_attrs.get(\"fit_seconds\")\n",
    "        if fit_seconds is not None:\n",
    "            record[\"fit_seconds\"] = fit_seconds\n",
    "        trial_rows.append(record)\n",
    "    trials_df = pd.DataFrame(trial_rows)\n",
    "    trials_path = OUTPUT_DIR / f\"optuna_trials_{target}.csv\"\n",
    "    if not trials_df.empty:\n",
    "        trials_df.to_csv(trials_path, index=False)\n",
    "    else:\n",
    "        trials_path.write_text(\"trial_number,value\")\n",
    "\n",
    "    optuna_reports[target] = {\n",
    "        \"best\": best_info,\n",
    "        \"best_params\": best_params,\n",
    "        \"metrics\": dataset_metric_map,\n",
    "        \"trials_csv\": trials_path,\n",
    "    }\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_records)\n",
    "metrics_path = OUTPUT_DIR / \"evaluation_metrics.csv\"\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "\n",
    "membership_df = pd.DataFrame(membership_records)\n",
    "membership_path = OUTPUT_DIR / \"membership_inference.csv\"\n",
    "membership_df.to_csv(membership_path, index=False)\n",
    "\n",
    "in_hospital_model = models.get(\"in_hospital_mortality\")\n",
    "if in_hospital_model is not None:\n",
    "    print(\"Generating synthetic data for TSTR/TRTR comparisons\u2026\")\n",
    "    X_train_full = prepare_features(train_df, FEATURE_COLUMNS)\n",
    "    y_train_full = train_df[\"in_hospital_mortality\"]\n",
    "    numeric_train = to_numeric_frame(X_train_full)\n",
    "\n",
    "    rng = np.random.default_rng(RANDOM_STATE)\n",
    "    synthetic_labels = rng.choice(\n",
    "        y_train_full, size=len(y_train_full), replace=True\n",
    "    )\n",
    "\n",
    "    synthetic_features = in_hospital_model.sample(\n",
    "        len(synthetic_labels), conditional=True, y=synthetic_labels\n",
    "    )\n",
    "    numeric_synthetic = to_numeric_frame(synthetic_features[FEATURE_COLUMNS])\n",
    "\n",
    "    numeric_test = to_numeric_frame(prepare_features(test_df, FEATURE_COLUMNS))\n",
    "    y_test = test_df[\"in_hospital_mortality\"]\n",
    "\n",
    "    tstr_metrics = evaluate_tstr(\n",
    "        (numeric_synthetic.to_numpy(), np.asarray(synthetic_labels)),\n",
    "        (numeric_test.to_numpy(), y_test.to_numpy()),\n",
    "        make_logistic_pipeline,\n",
    "    )\n",
    "    trtr_metrics = evaluate_trtr(\n",
    "        (numeric_train.to_numpy(), y_train_full.to_numpy()),\n",
    "        (numeric_test.to_numpy(), y_test.to_numpy()),\n",
    "        make_logistic_pipeline,\n",
    "    )\n",
    "    tstr_results = pd.DataFrame(\n",
    "        [\n",
    "            {\"setting\": \"TSTR\", **tstr_metrics},\n",
    "            {\"setting\": \"TRTR\", **trtr_metrics},\n",
    "        ]\n",
    "    )\n",
    "    tstr_path = OUTPUT_DIR / \"tstr_trtr_comparison.csv\"\n",
    "    tstr_results.to_csv(tstr_path, index=False)\n",
    "\n",
    "    distribution_rows: List[Dict[str, object]] = []\n",
    "    for column in FEATURE_COLUMNS:\n",
    "        real_values = numeric_train[column].to_numpy()\n",
    "        synthetic_values = numeric_synthetic[column].to_numpy()\n",
    "        distribution_rows.append(\n",
    "            {\n",
    "                \"feature\": column,\n",
    "                \"ks\": kolmogorov_smirnov_statistic(real_values, synthetic_values),\n",
    "                \"mmd\": rbf_mmd(real_values, synthetic_values, random_state=RANDOM_STATE),\n",
    "                \"mutual_information\": mutual_information_feature(\n",
    "                    real_values, synthetic_values\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "    distribution_df = pd.DataFrame(distribution_rows)\n",
    "    distribution_path = OUTPUT_DIR / \"distribution_shift_metrics.csv\"\n",
    "    distribution_df.to_csv(distribution_path, index=False)\n",
    "else:\n",
    "    print(\"Primary target model not available; skipping TSTR/TRTR and distribution analysis.\")\n",
    "\n",
    "\n",
    "summary_lines: List[str] = [\n",
    "    \"# Mortality modelling report\",\n",
    "    \"\",\n",
    "    \"## Schema\",\n",
    "    schema_table,\n",
    "    \"\",\n",
    "    \"## Model selection and performance\",\n",
    "]\n",
    "\n",
    "if not optuna_reports:\n",
    "    summary_lines.append(\"No models were trained by optuna.\")\n",
    "\n",
    "for target, report in optuna_reports.items():\n",
    "    best = report[\"best\"]\n",
    "    best_params = report[\"best_params\"]\n",
    "    metrics_map: Mapping[str, Dict[str, float]] = report[\"metrics\"]\n",
    "    summary_lines.append(f\"### {target}\")\n",
    "    best_value = best.get(\"value\")\n",
    "    value_text = (\n",
    "        f\"{best_value:.4f}\" if isinstance(best_value, (int, float)) else \"n/a\"\n",
    "    )\n",
    "    summary_lines.append(\n",
    "        f\"Best Optuna trial #{best.get('trial_number')} with validation ROAUC {value_text}\"\n",
    "    )\n",
    "    summary_lines.append(\"Best parameters:\")\n",
    "    summary_lines.append(\"```json\")\n",
    "    summary_lines.append(json.dumps(best_params, indent=2, ensure_ascii=False))\n",
    "    summary_lines.append(\"```\")\n",
    "    summary_lines.append(\"| Dataset | AUC | ACC | SPE | SEN | Brier |\")\n",
    "    summary_lines.append(\"| --- | --- | --- | --- | --- | --- |\")\n",
    "    for dataset_name, metrics in metrics_map.items():\n",
    "        summary_lines.append(\n",
    "            \"| {dataset} | {auc} | {acc} | {spe} | {sen} | {brier} |\".format(\n",
    "                dataset=dataset_name,\n",
    "                auc=format_float(metrics.get(\"AUC\")),\n",
    "                acc=format_float(metrics.get(\"ACC\")),\n",
    "                spe=format_float(metrics.get(\"SPE\")),\n",
    "                sen=format_float(metrics.get(\"SEN\")),\n",
    "                brier=format_float(metrics.get(\"Brier\")),\n",
    "            )\n",
    "        )\n",
    "    summary_lines.append(\n",
    "        f\"Optuna trials logged at: {report['trials_csv'].relative_to(OUTPUT_DIR)}\"\n",
    "    )\n",
    "    summary_lines.append(\n",
    "        f\"Calibration plot: {calibration_paths[target].relative_to(OUTPUT_DIR)}\"\n",
    "    )\n",
    "    summary_lines.append(\n",
    "        f\"Latent projection: {latent_paths[target].relative_to(OUTPUT_DIR)}\"\n",
    "    )\n",
    "    summary_lines.append(\"\")\n",
    "\n",
    "if tstr_results is not None:\n",
    "    summary_lines.append(\"## TSTR vs TRTR\")\n",
    "    summary_lines.append(\"| Setting | Accuracy | AUROC | AUPRC | Brier | ECE |\")\n",
    "    summary_lines.append(\"| --- | --- | --- | --- | --- | --- |\")\n",
    "    for _, row in tstr_results.iterrows():\n",
    "        summary_lines.append(\n",
    "            \"| {setting} | {acc:.3f} | {auroc:.3f} | {auprc:.3f} | {brier:.3f} | {ece:.3f} |\".format(\n",
    "                setting=row[\"setting\"],\n",
    "                acc=row.get(\"accuracy\", np.nan),\n",
    "                auroc=row.get(\"auroc\", row.get(\"auc\", np.nan)),\n",
    "                auprc=row.get(\"auprc\", np.nan),\n",
    "                brier=row.get(\"brier\", np.nan),\n",
    "                ece=row.get(\"ece\", np.nan),\n",
    "            )\n",
    "        )\n",
    "    summary_lines.append(\"\")\n",
    "\n",
    "summary_lines.append(\"## Distribution shift and privacy\")\n",
    "if distribution_df is not None and distribution_path is not None:\n",
    "    distribution_top = distribution_df.sort_values(\"ks\", ascending=False).head(10)\n",
    "    summary_lines.append(\"Top 10 features by KS statistic:\")\n",
    "    summary_lines.append(\"| Feature | KS | MMD | Mutual information |\")\n",
    "    summary_lines.append(\"| --- | --- | --- | --- |\")\n",
    "    for _, row in distribution_top.iterrows():\n",
    "        summary_lines.append(\n",
    "            \"| {feature} | {ks:.3f} | {mmd:.3f} | {mi:.3f} |\".format(\n",
    "                feature=row[\"feature\"],\n",
    "                ks=row.get(\"ks\", np.nan),\n",
    "                mmd=row.get(\"mmd\", np.nan),\n",
    "                mi=row.get(\"mutual_information\", np.nan),\n",
    "            )\n",
    "        )\n",
    "    summary_lines.append(\n",
    "        f\"Full distribution metrics: {distribution_path.relative_to(OUTPUT_DIR)}\"\n",
    "    )\n",
    "else:\n",
    "    summary_lines.append(\"Distribution metrics were not computed.\")\n",
    "\n",
    "if membership_df.empty:\n",
    "    summary_lines.append(\"No membership inference metrics were recorded.\")\n",
    "else:\n",
    "    summary_lines.append(\"Membership inference results:\")\n",
    "    summary_lines.append(\n",
    "        \"| Target | Attack AUC | Best accuracy | Threshold | Majority baseline |\"\n",
    "    )\n",
    "    summary_lines.append(\"| --- | --- | --- | --- | --- |\")\n",
    "    for _, row in membership_df.iterrows():\n",
    "        summary_lines.append(\n",
    "            \"| {target} | {auc:.3f} | {best_acc:.3f} | {threshold:.3f} | {majority:.3f} |\".format(\n",
    "                target=row[\"target\"],\n",
    "                auc=row.get(\"attack_auc\", np.nan),\n",
    "                best_acc=row.get(\"attack_best_accuracy\", np.nan),\n",
    "                threshold=row.get(\"attack_best_threshold\", np.nan),\n",
    "                majority=row.get(\"attack_majority_class_accuracy\", np.nan),\n",
    "            )\n",
    "        )\n",
    "    summary_lines.append(\n",
    "        f\"Membership metrics saved to: {membership_path.relative_to(OUTPUT_DIR)}\"\n",
    "    )\n",
    "\n",
    "summary_path = OUTPUT_DIR / \"summary.md\"\n",
    "summary_path.write_text(\"\\n\".join(summary_lines), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Analysis complete.\")\n",
    "print(f\"Metric table saved to {metrics_path}\")\n",
    "for target, path in calibration_paths.items():\n",
    "    print(f\"Calibration plot for {target}: {path}\")\n",
    "for target, path in latent_paths.items():\n",
    "    print(f\"Latent space plot for {target}: {path}\")\n",
    "print(f\"Membership inference results saved to {membership_path}\")\n",
    "if (\n",
    "    in_hospital_model is not None\n",
    "    and tstr_path is not None\n",
    "    and distribution_path is not None\n",
    "):\n",
    "    print(f\"TSTR/TRTR comparison saved to {tstr_path}\")\n",
    "    print(f\"Distribution metrics saved to {distribution_path}\")\n",
    "print(f\"Summary written to {summary_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a53682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}